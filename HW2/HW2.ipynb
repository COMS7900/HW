{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUXuP0n8tnhr"
      },
      "source": [
        "Blazing Hawks\n",
        "Clifford Jones\\\n",
        "Dongyu Liu\\\n",
        "Yuan Chen\n",
        "\n",
        "\n",
        "**A.I. Disclaimer: Work for this assignment was completed with the aid of artificial intelligence tools.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Bisection Method\n",
        "\n",
        "#### **Overview**  \n",
        "Bisection Method is a numerical method used to find the roots of equations, based on the **Intermediate Value Theorem**.\n",
        "\n",
        "#### **Steps**  \n",
        "1. Choose an initial interval \\([a, b]\\) such that $( f(a) \\cdot f(b) < 0 )$ (i.e., the function has a root within the interval).\n",
        "2. Compute the midpoint $( c = \\frac{a + b}{2} )$.\n",
        "3. Evaluate \\( f(c) \\) and check:\n",
        "   - If $( f(c) = 0 )$, then $( c )$ is the root, and the algorithm stops.\n",
        "   - If $( f(a) \\cdot f(c) < 0 )$, set $( b = c )$; otherwise, set $( a = c )$.\n",
        "4. Repeat steps 2-3 until the error criterion is met.\n",
        "\n",
        "#### **Advantages**  \n",
        "- Guaranteed convergence if the initial interval is chosen correctly.  \n",
        "- Simple and easy to implement.  \n",
        "\n",
        "#### **Disadvantages**  \n",
        "- Converges slowly, often requiring many iterations.  \n",
        "- Only applicable to continuous functions and requires an initial interval containing a root.  \n",
        "\n",
        "---\n",
        "\n",
        "## Newton Method\n",
        "\n",
        "#### **Overview**  \n",
        "Newton Method is an iterative technique for approximating the roots of equations, based on **Taylor expansion** and **tangent approximation**.\n",
        "\n",
        "#### **Steps**  \n",
        "1. Select an initial guess $( x_0 )$.\n",
        "2. Compute the iteration formula:\n",
        "   $$\n",
        "   x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
        "   $$\n",
        "3. Update $( x_{n+1} )$ and repeat until the error criterion is met.\n",
        "\n",
        "#### **Advantages**  \n",
        "- Fast convergence (typically quadratic, meaning the error decreases quadratically).  \n",
        "- Applicable to a wide range of nonlinear equations.  \n",
        "\n",
        "#### **Disadvantages**  \n",
        "- Requires the derivative $( f'(x) )$, which can be complex to compute.  \n",
        "- May not converge or may oscillate/diverge if the initial guess is poorly chosen.  \n",
        "\n",
        "\n",
        "#### **Multivariable Newton Method with Hessian Matrix**  \n",
        "For functions with multiple variables, Newton's method extends using the **Hessian matrix**. The Hessian matrix \\( H(x) \\) is a square matrix of second-order partial derivatives, used to refine the root-finding process.\n",
        "\n",
        "The iteration formula for a function $( f: \\mathbb{R}^n \\to \\mathbb{R}^n )$ becomes:\n",
        "   $$\n",
        "   x_{n+1} = x_n - H(x_n)^{-1} \\nabla f(x_n)\n",
        "   $$\n",
        "where:  \n",
        "- $( \\nabla f(x_n) )$ is the gradient vector (first-order partial derivatives).  \n",
        "- $( H(x_n) )$ is the Hessian matrix (second-order partial derivatives).  \n",
        "\n",
        "This method is particularly useful in **optimization problems**, where it helps find local minima or maxima efficiently. However, computing and inverting the Hessian matrix can be computationally expensive for high-dimensional problems.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39MIPC4atnhv"
      },
      "source": [
        "\\\n",
        "\\\n",
        "We first test these methods on a **single-variable function**, followed by **multi-variable cases**, including **linear and nonlinear functions**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Single Variable Case: Cube Root Function**\n",
        "We start by testing **Bisection and Newton’s methods** using the cube root function:\n",
        "$$\n",
        "f(x) = \\sqrt[3]{x}\n",
        "$$\n",
        "with first guess a = -1, b = 0.8\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHjzO8lEtnhw",
        "outputId": "47d53ce4-bbea-4a18-83b2-4885190ca7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bisection Method Iterations:\n",
            "converged after 27 iterations\n",
            "\n",
            "Bisection method result: 7.450580843640056e-10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def f(x):\n",
        "    # Cube root function, works for negative x too.\n",
        "    return np.cbrt(x)\n",
        "\n",
        "\n",
        "def bisection_method(a, b, tol=1e-8, max_iter=100):\n",
        "    if f(a)*f(b) >= 0:\n",
        "        print(\"Bisection method fails: f(a) and f(b) must have opposite signs.\")\n",
        "        return None\n",
        "    print(\"\\nBisection Method Iterations:\")\n",
        "    for i in range(max_iter):\n",
        "        c = (a + b) / 2.0\n",
        "        fc = f(c)\n",
        "        if abs(fc) < tol or (b - a)/2 < tol:\n",
        "            print(\"converged after\", i, \"iterations\")\n",
        "            return c\n",
        "        if f(a)*fc < 0:\n",
        "            b = c\n",
        "        else:\n",
        "            a = c\n",
        "    return (a + b) / 2.0\n",
        "\n",
        "\n",
        "# Bisection method: choose an interval that brackets the root.\n",
        "a, b = -1, 0.8  # f(-1) = -1, f(1) = 1 so the sign change condition holds.\n",
        "bisection_root = bisection_method(a, b)\n",
        "print(\"\\nBisection method result:\", bisection_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. Implementing Newton’s Method**\n",
        "Newton’s Method is a fast, **quadratic convergence** approach for finding roots. It iteratively refines the root approximation using:\n",
        "$$\n",
        "x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n",
        "$$\n",
        "where $( f'(x) )$ is the derivative of $( f(x) )$.\n",
        "\n",
        "For the **cube root function**:\n",
        "$$\n",
        "f(x) = \\sqrt[3]{x}\n",
        "$$\n",
        "its derivative is:\n",
        "$$\n",
        "f'(x) = \\frac{1}{3 \\cdot |x|^{2/3}}\n",
        "$$\n",
        "\n",
        "However, $( f'(x) )$ **diverges to infinity at $( x = 0 )$**, which can cause **Newton’s method to fail**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Newton's Method Iterations:\n",
            "Iteration 0: x = -2.0\n",
            "Iteration 1: x = 4.000000000000001\n",
            "Iteration 2: x = -8.0\n",
            "Iteration 3: x = 16.0\n",
            "Iteration 4: x = -32.00000000000001\n",
            "Iteration 5: x = 64.0\n",
            "Iteration 6: x = -128.0\n",
            "Iteration 7: x = 256.00000000000006\n",
            "Iteration 8: x = -512.0\n",
            "Iteration 9: x = 1024.0\n",
            "Iteration 10: x = -2048.0000000000005\n",
            "Iteration 11: x = 4096.0\n",
            "Iteration 12: x = -8192.0\n",
            "Iteration 13: x = 16384.000000000004\n",
            "Iteration 14: x = -32768.0\n",
            "Iteration 15: x = 65536.0\n",
            "Iteration 16: x = -131072.00000000003\n",
            "Iteration 17: x = 262144.0\n",
            "Iteration 18: x = -524288.0\n",
            "Iteration 19: x = 1048576.0000000002\n",
            "\n",
            "Newton's method result: 1048576.0000000002\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def fprime(x):\n",
        "    # Derivative: 1/(3*(|x|^(2/3))).\n",
        "    # Avoid division by zero.\n",
        "    if x == 0:\n",
        "        raise ZeroDivisionError(\"Derivative undefined at x=0\")\n",
        "    return 1 / (3 * np.cbrt(x**2))\n",
        "\n",
        "def newton_method(x0, tol=1e-8, max_iter=20):\n",
        "    x = x0\n",
        "    print(\"Newton's Method Iterations:\")\n",
        "    for i in range(max_iter):\n",
        "        try:\n",
        "            fp = fprime(x)\n",
        "        except ZeroDivisionError:\n",
        "            print(f\"Iteration {i}: Derivative zero at x={x}.\")\n",
        "            return None\n",
        "        x_new = x - f(x) / fp\n",
        "        print(f\"Iteration {i}: x = {x_new}\")\n",
        "        if abs(x_new - x) < tol:\n",
        "            print(f\"Converged after {i} iterations.\")\n",
        "            return x_new\n",
        "        x = x_new\n",
        "    return x\n",
        "\n",
        "# For f(x) = cbrt(x), the only root is at x = 0.\n",
        "# Newton's method starting from x0 != 0 will not converge.\n",
        "x0 = 1.0\n",
        "newton_root = newton_method(x0)\n",
        "print(\"\\nNewton's method result:\", newton_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Newton's Method for Linear Regression**\n",
        "\n",
        "In this section, we apply **Newton’s Method** to estimate the parameters $( \\beta )$ in a **linear regression model**. The goal is to minimize the sum of squared residuals:\n",
        "\n",
        "$$\n",
        "\\min_{\\beta} \\| X\\beta - y \\|^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $( X )$ is an $( m \\times n )$ matrix of input features.\n",
        "- $( \\beta )$ is an $( n \\times 1 )$ vector of unknown parameters.\n",
        "- $( y )$ is an $( m \\times 1 )$ vector of observed target values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m, n = 100, 3\n",
        "X = np.random.rand(m, n)\n",
        "true_beta = np.array([3,5,6])\n",
        "y = X @ true_beta + np.random.randn(m)*0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Implementing Newton’s Method**\n",
        "\n",
        "### **Algorithm for Newton’s Method in Linear Regression**\n",
        "Newton’s Method iteratively updates the estimate of $ \\beta $ using the formula:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - H^{-1} \\nabla L(\\beta_k)\n",
        "$$\n",
        "\n",
        "For **linear least squares regression**, the loss function is:\n",
        "\n",
        "$$\n",
        "L(\\beta) = \\frac{1}{2} \\|X\\beta - y\\|^2\n",
        "$$\n",
        "\n",
        "- **Gradient (First Derivative):**\n",
        "  $$\n",
        "  \\nabla L(\\beta) = X^T (X\\beta - y)\n",
        "  $$\n",
        "- **Hessian (Second Derivative):**\n",
        "  $$\n",
        "  H = X^T X\n",
        "  $$\n",
        "\n",
        "Since this is a **quadratic loss function**, Newton’s update rule becomes:\n",
        "\n",
        "$$\n",
        "\\beta_{k+1} = \\beta_k - (X^T X)^{-1} X^T (X\\beta_k - y)\n",
        "$$\n",
        "\n",
        "This means that Newton’s update step is equivalent to solving for $\\beta$ using the **normal equation**.\n",
        "\n",
        "\n",
        "### **Why Converge in one step?**\n",
        "Since the Newton update step is:\n",
        "\n",
        "$$\n",
        "\\beta_{new} = \\beta - (X^T X)^{-1} X^T (X\\beta - y)\n",
        "$$\n",
        "\n",
        "If we start at $ \\beta = 0 $:\n",
        "\n",
        "$$\n",
        "\\beta_{new} = - (X^T X)^{-1} X^T (-y)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\beta_{new} = (X^T X)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "which is the **closed-form solution for linear regression** using the normal equation:\n",
        "\n",
        "$$\n",
        "\\beta^* = (X^T X)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "Thus, Newton’s method **immediately finds the optimal solution in one step**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 iteration: beta = [1. 1. 1.]\n",
            "1 iteration: beta = [3.01124533 4.97041046 6.03920518]\n",
            "True beta: [3 5 6]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def Newton_method(tol=1e-8, max_iter=20):\n",
        "    beta = np.zeros(n)\n",
        "    for i in range(max_iter):\n",
        "        print(f\"{i} iteration: beta = {beta}\")\n",
        "        gradient = X.T @ (X @ beta - y)\n",
        "        hessian = X.T @ X\n",
        "        beta -= np.linalg.solve(hessian, gradient)\n",
        "        if np.linalg.norm(gradient) < tol:\n",
        "            return beta\n",
        "\n",
        "    print(\"Newton's method did not converge.\")\n",
        "    return beta\n",
        "\n",
        "beta = Newton_method()\n",
        "# print(\"Estimated beta:\", beta)\n",
        "print(\"True beta:\", true_beta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Newton's Method for Logistic Regression**\n",
        "\n",
        "This session explains the implementation of **Newton's Method** for **logistic regression**, including:\n",
        "- How the **Hessian matrix** is computed.\n",
        "- Why the **condition number of the Hessian** is checked.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### **1. Sigmoid Function**\n",
        "The **sigmoid function** is used to transform linear outputs into probabilities:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "\n",
        "It maps any real number to the range $ (0,1) $, ensuring that outputs can be interpreted as probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Logistic regression**\n",
        "Logistic regression models the probability of class **$ y = 1 $** given input **$ X $** using the **sigmoid function**:\n",
        "\n",
        "$$\n",
        "P(y = 1 | X) = \\sigma(X\\theta) = \\frac{1}{1 + e^{-X\\theta}}\n",
        "$$\n",
        "\n",
        "Given a dataset with $ m $ samples and $ n $ features, we assume that each label $ y_i $ is generated independently according to the Bernoulli distribution:\n",
        "\n",
        "$$\n",
        "y_i \\sim \\text{Bernoulli}(\\sigma(X_i^T \\theta))\n",
        "$$\n",
        "\n",
        "This means the probability mass function (PMF) for each sample is:\n",
        "\n",
        "$$\n",
        "P(y_i | X_i, \\theta) = \\sigma(X_i^T \\theta)^{y_i} (1 - \\sigma(X_i^T \\theta))^{(1 - y_i)}\n",
        "$$\n",
        "\n",
        "The **likelihood function** for the entire dataset, assuming **independent observations**, is the product of individual sample probabilities:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\prod_{i=1}^{m} P(y_i | X_i, \\theta)\n",
        "= \\prod_{i=1}^{m} \\sigma(X_i^T \\theta)^{y_i} (1 - \\sigma(X_i^T \\theta))^{(1 - y_i)}\n",
        "$$\n",
        "\n",
        "Since likelihood functions are often more convenient to work with in **log space**, we take the **log-likelihood**:\n",
        "\n",
        "$$\n",
        "\\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{m} \\left[ y_i \\log \\sigma(X_i^T \\theta) + (1 - y_i) \\log (1 - \\sigma(X_i^T \\theta)) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\sigma(X_i^T \\theta) $ is the predicted probability of $ y_i = 1 $.\n",
        "- $ \\log \\sigma(X_i^T \\theta) $ measures confidence in classifying positive samples correctly.\n",
        "- $ \\log (1 - \\sigma(X_i^T \\theta)) $ measures confidence in classifying negative samples correctly.\n",
        "\n",
        "Thus, **maximizing the log-likelihood function** corresponds to **maximizing the probability of observing the given labels** under the logistic regression model.\n",
        "\n",
        "\n",
        "---\n",
        "### **3. Gradient Computation**\n",
        "To apply **Newton’s method**, we first compute the **gradient** of $ L(\\theta) $ with respect to $ \\theta $:\n",
        "\n",
        "$$\n",
        "\\nabla L(\\theta) = \\sum_{i=1}^{m} \\left[ y_i - \\sigma(X_i^T \\theta) \\right] X_i\n",
        "$$\n",
        "\n",
        "which can be rewritten in **matrix form** as:\n",
        "\n",
        "$$\n",
        "\\nabla L(\\theta) = X^T (y - h)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ X $ is the $ m \\times n $ **feature matrix**.\n",
        "- $ y $ is the $ m \\times 1 $ **vector of actual labels**.\n",
        "- $ h = \\sigma(X\\theta) $ is the $ m \\times 1 $ **vector of predicted probabilities**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Hessian Matrix Computation**\n",
        "The **Hessian matrix** is the second derivative of the **log-likelihood function** with respect to $ \\theta $:\n",
        "\n",
        "$$\n",
        "H = \\frac{\\partial^2 L(\\theta)}{\\partial \\theta^2}\n",
        "$$\n",
        "\n",
        "Since the gradient is:\n",
        "\n",
        "$$\n",
        "\\nabla L(\\theta) = X^T (y - h)\n",
        "$$\n",
        "\n",
        "We differentiate again:\n",
        "\n",
        "$$\n",
        "H = \\frac{\\partial}{\\partial \\theta} \\left[ X^T (y - h) \\right]\n",
        "$$\n",
        "\n",
        "Since $ y $ is constant with respect to $ \\theta $, we only need to differentiate $ -X^T h $, where:\n",
        "\n",
        "$$\n",
        "h = \\sigma(X\\theta)\n",
        "$$\n",
        "\n",
        "Using the **derivative of the sigmoid function**:\n",
        "\n",
        "$$\n",
        "\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))\n",
        "$$\n",
        "\n",
        "Applying the chain rule:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h}{\\partial \\theta} = \\text{diag} (h \\odot (1 - h)) X\n",
        "$$\n",
        "\n",
        "where $ \\odot $ represents element-wise multiplication.\n",
        "\n",
        "Thus, differentiating again:\n",
        "\n",
        "$$\n",
        "H = -X^T \\left[ \\text{diag} (h \\odot (1 - h)) X \\right]\n",
        "$$\n",
        "\n",
        "which simplifies to:\n",
        "\n",
        "$$\n",
        "H = X^T R X\n",
        "$$\n",
        "\n",
        "where $ R $ is the **diagonal matrix**:\n",
        "\n",
        "$$\n",
        "R_{ii} = h_i (1 - h_i) = \\sigma(X_i^T \\theta) (1 - \\sigma(X_i^T \\theta))\n",
        "$$\n",
        "\n",
        "This is the **Hessian matrix for logistic regression**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Checking Hessian Condition Number**\n",
        "The **condition number** of $ H $ is computed to determine numerical stability:\n",
        "\n",
        "$$\n",
        "\\text{condition number} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n",
        "$$\n",
        "\n",
        "where $ \\lambda_{\\max} $ and $ \\lambda_{\\min} $ are the **largest and smallest eigenvalues** of $ H $. \n",
        "\n",
        "A **high condition number** indicates:\n",
        "- The Hessian is **ill-conditioned**.\n",
        "- **Large numerical errors** may occur when inverting $ H $.\n",
        "\n",
        "If the **condition number is too high**, the optimization might fail.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def newton_method(X, y, tol=1e-6, max_iter=20):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)  # initialize guess at 0\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        h = sigmoid(X @ theta)\n",
        "        \n",
        "        # get gradient and Hessian matrix\n",
        "        gradient = X.T @ (y - h)\n",
        "        \n",
        "        R = np.diag(h * (1 - h))  # m x m diagonal matrix\n",
        "        H = X.T @ R @ X  # Hessian matrix\n",
        "\n",
        "        condition_number = np.linalg.cond(H)\n",
        "        print(f\"Hessian condition number for {i+1}th iterations:\", condition_number)\n",
        "\n",
        "        # update theta\n",
        "        try:\n",
        "            delta_theta = np.linalg.solve(H, gradient)  \n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Singular matrix error!\")\n",
        "        theta += delta_theta\n",
        "\n",
        "        # check convergence\n",
        "        if np.linalg.norm(delta_theta) < tol:\n",
        "            print(f\"Converged in {i+1} iterations.\")\n",
        "            return theta\n",
        "    \n",
        "    print(\"Newton's method did not fully converge.\")\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hessian condition number for 1th iterations: 6.719390237522599\n",
            "Hessian condition number for 2th iterations: 5.07764089617307\n",
            "Hessian condition number for 3th iterations: 4.235696970583115\n",
            "Hessian condition number for 4th iterations: 4.001463243527199\n",
            "Hessian condition number for 5th iterations: 4.173746458969424\n",
            "Hessian condition number for 6th iterations: 4.304147668309576\n",
            "Hessian condition number for 7th iterations: 4.314402827265504\n",
            "Hessian condition number for 8th iterations: 4.314439671733583\n",
            "Converged in 8 iterations.\n",
            "Estimated theta: [2.79584111 4.97753775]\n"
          ]
        }
      ],
      "source": [
        "# success case\n",
        "np.random.seed(42)\n",
        "m, n = 5000, 2 # 100 samples, 2 features\n",
        "X = np.random.rand(m, n)\n",
        "true_theta = np.array([3, 5]) # true parameters\n",
        "y_prob = 1 / (1 + np.exp(-X @ true_theta))  # logistic function\n",
        "y = (np.random.rand(m) < y_prob).astype(int)  \n",
        "theta_estimated = newton_method(X, y)\n",
        "print(\"Estimated theta:\", theta_estimated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hessian condition number for 1th iterations: 254.794196305102\n",
            "Hessian condition number for 2th iterations: 254.34604162860165\n",
            "Hessian condition number for 3th iterations: 253.05370293741206\n",
            "Hessian condition number for 4th iterations: 249.56651290247902\n",
            "Hessian condition number for 5th iterations: 240.5976250581648\n",
            "Hessian condition number for 6th iterations: 219.7669786371672\n",
            "Hessian condition number for 7th iterations: 181.88194995153088\n",
            "Hessian condition number for 8th iterations: 141.6684792059006\n",
            "Hessian condition number for 9th iterations: 130.72022121990454\n",
            "Hessian condition number for 10th iterations: 148.6894125377634\n",
            "Hessian condition number for 11th iterations: 169.38276635851625\n",
            "Hessian condition number for 12th iterations: 176.1563056138908\n",
            "Hessian condition number for 13th iterations: 176.57262273954888\n",
            "Hessian condition number for 14th iterations: 176.57401115639712\n",
            "Converged in 14 iterations.\n",
            "Estimated theta: [-3.66805674  5.78143667]\n"
          ]
        }
      ],
      "source": [
        "# failure case\n",
        "np.random.seed(42)\n",
        "m, n = 5000, 2\n",
        "X = np.random.rand(m, 1)  \n",
        "# second feature is twice the first plus 1, so hesse matrix is singular\n",
        "X = np.hstack([X, 2 * X + 1])  \n",
        "\n",
        "true_theta = np.array([3, 5])\n",
        "y_prob = 1 / (1 + np.exp(-X @ true_theta))  \n",
        "y = (np.random.rand(m) < y_prob).astype(int)\n",
        "\n",
        "theta_estimated = newton_method(X, y)\n",
        "print(\"Estimated theta:\", theta_estimated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIcG47_Ptnhx"
      },
      "source": [
        "## QR decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHq1gs6wtnhx"
      },
      "source": [
        "### Gram-Schmidt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gram_schmidt(A):\n",
        "    \"\"\"\n",
        "    Compute the QR factorization of A using the classical Gram–Schmidt process.\n",
        "    Returns:\n",
        "       Q: m x n matrix with orthonormal columns\n",
        "       R: n x n upper triangular matrix, with A = Q @ R\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "    Q = np.zeros((m, n))\n",
        "    R = np.zeros((n, n))\n",
        "    for j in range(n):\n",
        "        v = A[:, j].copy()\n",
        "        for i in range(j):\n",
        "            R[i, j] = np.dot(Q[:, i], A[:, j])\n",
        "            v = v - R[i, j] * Q[:, i]\n",
        "        R[j, j] = np.linalg.norm(v)\n",
        "        if np.isclose(R[j, j], 0.0):\n",
        "            raise ValueError(\"Matrix A has linearly dependent columns.\")\n",
        "        Q[:, j] = v / R[j, j]\n",
        "    return Q, R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIVIWHwetnhy",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### Householder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def householder_qr(A):\n",
        "    \"\"\"\n",
        "    Compute the QR factorization of A using Householder reflections.\n",
        "    Returns:\n",
        "       Q: m x m orthogonal matrix\n",
        "       R: m x n upper triangular matrix, with A = Q @ R\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "    R = A.copy().astype(float)\n",
        "    Q = np.eye(m)\n",
        "\n",
        "    for k in range(min(m, n)):\n",
        "        # Form the vector to reflect\n",
        "        x = R[k:, k]\n",
        "        norm_x = np.linalg.norm(x)\n",
        "        if np.isclose(norm_x, 0):\n",
        "            continue\n",
        "        # Choose sign to avoid cancellation\n",
        "        sign = -np.sign(x[0]) if x[0] != 0 else -1\n",
        "        u1 = x[0] - sign * norm_x\n",
        "        v = x.copy()\n",
        "        v[0] = u1\n",
        "        v = v / np.linalg.norm(v)\n",
        "\n",
        "        # Build the Householder matrix H_k for the submatrix\n",
        "        Hk = np.eye(m)\n",
        "        Hk[k:, k:] -= 2.0 * np.outer(v, v)\n",
        "\n",
        "        R = Hk @ R\n",
        "        Q = Q @ Hk\n",
        "\n",
        "    return Q, R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WQ5IFeFtnhy"
      },
      "source": [
        "### Given Rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk2ulunutnhz",
        "outputId": "4764021d-5534-43fa-faa7-28856352b6dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def givens_qr(A):\n",
        "    \"\"\"\n",
        "    Compute the QR factorization of A using Givens rotations.\n",
        "    Returns:\n",
        "       Q: m x m orthogonal matrix\n",
        "       R: m x n upper triangular matrix, with A = Q @ R\n",
        "    \"\"\"\n",
        "    m, n = A.shape\n",
        "    R = A.copy().astype(float)\n",
        "    Q = np.eye(m)\n",
        "\n",
        "    # Process each column\n",
        "    for j in range(n):\n",
        "        # Zero out the entries below the diagonal in column j\n",
        "        for i in range(j+1, m):\n",
        "            a = R[j, j]\n",
        "            b = R[i, j]\n",
        "            r = np.hypot(a, b)\n",
        "            if np.isclose(r, 0):\n",
        "                continue\n",
        "            c = a / r\n",
        "            s = b / r\n",
        "            # Apply rotation to R: update rows j and i for columns j:n\n",
        "            for k in range(j, n):\n",
        "                temp = c * R[j, k] + s * R[i, k]\n",
        "                R[i, k] = -s * R[j, k] + c * R[i, k]\n",
        "                R[j, k] = temp\n",
        "            # Apply rotation to Q: update columns j and i of Q\n",
        "            temp_j = Q[:, j].copy()\n",
        "            temp_i = Q[:, i].copy()\n",
        "            Q[:, j] = c * temp_j + s * temp_i\n",
        "            Q[:, i] = -s * temp_j + c * temp_i\n",
        "    return Q, R\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matrix A:\n",
            "[[ 1.76405235  0.40015721  0.97873798]\n",
            " [ 2.2408932   1.86755799 -0.97727788]\n",
            " [ 0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351]]\n",
            "\n",
            "--- Gram-Schmidt QR Factorization ---\n",
            "Q (Gram-Schmidt):\n",
            "[[ 0.581441   -0.47915974  0.25929548]\n",
            " [ 0.73861028  0.64154205 -0.15752491]\n",
            " [ 0.31315418 -0.59551882 -0.46848788]\n",
            " [ 0.13533544 -0.06470756  0.82974144]]\n",
            "R (Gram-Schmidt):\n",
            "[[ 3.0339318   1.58416139  0.01174224]\n",
            " [ 0.          1.08719312 -1.12857042]\n",
            " [ 0.          0.          1.66275572]]\n",
            "Reconstruction (Q_gs @ R_gs):\n",
            "[[ 1.76405235  0.40015721  0.97873798]\n",
            " [ 2.2408932   1.86755799 -0.97727788]\n",
            " [ 0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351]]\n",
            "\n",
            "--- Householder QR Factorization ---\n",
            "Q (Householder, reduced to first n columns):\n",
            "[[-0.581441    0.47915974  0.25929548]\n",
            " [-0.73861028 -0.64154205 -0.15752491]\n",
            " [-0.31315418  0.59551882 -0.46848788]\n",
            " [-0.13533544  0.06470756  0.82974144]]\n",
            "R (Householder):\n",
            "[[-3.03393180e+00 -1.58416139e+00 -1.17422449e-02]\n",
            " [ 3.05366741e-16 -1.08719312e+00  1.12857042e+00]\n",
            " [ 7.93659740e-17 -8.71248884e-17  1.66275572e+00]\n",
            " [-2.60528696e-16  1.26163761e-16 -1.96074224e-16]]\n",
            "Reconstruction (Q_h_full @ R_h):\n",
            "[[ 1.76405235  0.40015721  0.97873798]\n",
            " [ 2.2408932   1.86755799 -0.97727788]\n",
            " [ 0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351]]\n",
            "\n",
            "--- Givens QR Factorization ---\n",
            "Q (Givens, reduced to first n columns):\n",
            "[[ 0.581441   -0.47915974  0.25929548]\n",
            " [ 0.73861028  0.64154205 -0.15752491]\n",
            " [ 0.31315418 -0.59551882 -0.46848788]\n",
            " [ 0.13533544 -0.06470756  0.82974144]]\n",
            "R (Givens):\n",
            "[[ 3.03393180e+00  1.58416139e+00  1.17422449e-02]\n",
            " [ 0.00000000e+00  1.08719312e+00 -1.12857042e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  1.66275572e+00]\n",
            " [ 5.55111512e-17  1.38777878e-17  0.00000000e+00]]\n",
            "Reconstruction (Q_giv_full @ R_giv):\n",
            "[[ 1.76405235  0.40015721  0.97873798]\n",
            " [ 2.2408932   1.86755799 -0.97727788]\n",
            " [ 0.95008842 -0.15135721 -0.10321885]\n",
            " [ 0.4105985   0.14404357  1.45427351]]\n",
            "\n",
            "--- Column-by-Column Comparison of Q (up to sign differences) ---\n",
            "\n",
            "Column 0:\n",
            "Gram-Schmidt vs. Householder: Agree\n",
            "Gram-Schmidt vs. Givens: Agree\n",
            "\n",
            "Column 1:\n",
            "Gram-Schmidt vs. Householder: Agree\n",
            "Gram-Schmidt vs. Givens: Agree\n",
            "\n",
            "Column 2:\n",
            "Gram-Schmidt vs. Householder: Agree\n",
            "Gram-Schmidt vs. Givens: Agree\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Main script to compute and compare QR factorizations\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(0)\n",
        "    A = np.random.randn(4, 3)\n",
        "\n",
        "    print(\"Matrix A:\")\n",
        "    print(A)\n",
        "\n",
        "    # 1. Gram-Schmidt QR factorization (reduced Q: m x n)\n",
        "    Q_gs, R_gs = gram_schmidt(A)\n",
        "    print(\"\\n--- Gram-Schmidt QR Factorization ---\")\n",
        "    print(\"Q (Gram-Schmidt):\")\n",
        "    print(Q_gs)\n",
        "    print(\"R (Gram-Schmidt):\")\n",
        "    print(R_gs)\n",
        "    print(\"Reconstruction (Q_gs @ R_gs):\")\n",
        "    print(Q_gs @ R_gs)\n",
        "\n",
        "    # 2. Householder QR factorization (full Q: m x m)\n",
        "    Q_h_full, R_h = householder_qr(A)\n",
        "    # Extract the first n columns for comparison\n",
        "    Q_h = Q_h_full[:, :A.shape[1]]\n",
        "    print(\"\\n--- Householder QR Factorization ---\")\n",
        "    print(\"Q (Householder, reduced to first n columns):\")\n",
        "    print(Q_h)\n",
        "    print(\"R (Householder):\")\n",
        "    print(R_h)\n",
        "    print(\"Reconstruction (Q_h_full @ R_h):\")\n",
        "    print(Q_h_full @ R_h)\n",
        "\n",
        "    # 3. Givens QR factorization (full Q: m x m)\n",
        "    Q_giv_full, R_giv = givens_qr(A)\n",
        "    # Extract the first n columns for comparison\n",
        "    Q_giv = Q_giv_full[:, :A.shape[1]]\n",
        "    print(\"\\n--- Givens QR Factorization ---\")\n",
        "    print(\"Q (Givens, reduced to first n columns):\")\n",
        "    print(Q_giv)\n",
        "    print(\"R (Givens):\")\n",
        "    print(R_giv)\n",
        "    print(\"Reconstruction (Q_giv_full @ R_giv):\")\n",
        "    print(Q_giv_full @ R_giv)\n",
        "\n",
        "    # Compare the Q matrices column by column (up to a possible sign change)\n",
        "    n = A.shape[1]\n",
        "    def same_up_to_sign(u, v):\n",
        "        return np.allclose(u, v) or np.allclose(u, -v)\n",
        "\n",
        "    print(\"\\n--- Column-by-Column Comparison of Q (up to sign differences) ---\")\n",
        "    for i in range(n):\n",
        "        col_gs = Q_gs[:, i]\n",
        "        col_hh = Q_h[:, i]\n",
        "        col_giv = Q_giv[:, i]\n",
        "        print(f\"\\nColumn {i}:\")\n",
        "        print(\"Gram-Schmidt vs. Householder:\", \"Agree\" if same_up_to_sign(col_gs, col_hh) else \"Differ\")\n",
        "        print(\"Gram-Schmidt vs. Givens:\", \"Agree\" if same_up_to_sign(col_gs, col_giv) else \"Differ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Research_Field_01",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
